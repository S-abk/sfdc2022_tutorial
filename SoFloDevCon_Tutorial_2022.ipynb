{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML on the Edge: Turning your RPi into an AI in under 45 minutes\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mpcrlab/sfdc2022_tutorial/blob/main/SoFloDevCon_Tutorial_2022.ipynb)",
        "\n",
        "\n",
        "This is the training code for the tutorial given at [SoFlo Dev Con 2022](https://techhubsouthflorida.org/meetups/soflodevcon/) by Misha Klopukh.\n",
        "\n",
        "In this notebook, there is everything you need to create a CNN classifier model to run on a Raspberry Pi. It will cover:\n",
        "- Generating a dataset from Bing Image search,\n",
        "- Getting a pretrained neural network from Pytorch,\n",
        "- Finetuning the model for the generated dataset,\n",
        "- Evaluating the model performance on test images,\n",
        "- Quantizing and fusing the model for fast inference, and\n",
        "- Exporting the model for use in our Raspberry Pi.\n",
        "\n",
        "The complete tutorial resourses can be found at <https://github.com/mpcrlab/sfdc2022_tutorial>"
      ],
      "metadata": {
        "id": "0MQy3tHnpQt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries we need"
      ],
      "metadata": {
        "id": "QvMtZd9Q7TV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy is a python library for n-dimensional array manipulation. It is pretty much ubiquitous in any numeric or scientific python code."
      ],
      "metadata": {
        "id": "Pkx70mNOVs0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YRQ8LFhg7ZuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matplotlib is a plotting library for python. It lets us plot images and graphs of various kinds."
      ],
      "metadata": {
        "id": "jwshOCHUVsWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sEGRU2CdVsMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is a deep learning framework which we will be using for this tutorial. Torchvision is an extension to PyTorch providing tools and models for vision tasks such as image classification."
      ],
      "metadata": {
        "id": "b8qt_nZHVrsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "8HqLmrEMVrdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tqdm library is used to display progress bars.\n",
        "\n",
        "We use `tqdm.notebook` here, but you should just use `tqdm` if you are not in a notebook environment."
      ],
      "metadata": {
        "id": "JU8-0k14M1cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import trange"
      ],
      "metadata": {
        "id": "QBqW7dS6M03t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting a Dataset"
      ],
      "metadata": {
        "id": "u5JhCW_WpwYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to create an image classifier, we must have a good dataset of examples to train our model on."
      ],
      "metadata": {
        "id": "OZh7JexcDJpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the data\n",
        "\n",
        "We will use Bing Image Search to quickly collect images of whatever categories we wish to classify."
      ],
      "metadata": {
        "id": "a27X5XiBrVYr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E1eV23o906a"
      },
      "outputs": [],
      "source": [
        "# We install the Bing Image Downloader library here\n",
        "# Do not run this if you already have it installed\n",
        "%pip install bing-image-downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i4USd_I9_uN"
      },
      "outputs": [],
      "source": [
        "from bing_image_downloader import downloader as image_getter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image Categories { display-mode: \"both\" }\n",
        "\n",
        "#@markdown Where should we put our downloaded images?\n",
        "DataDirectory = '/content/datasets/tutorial' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown What image categories should we search for?\n",
        "Category1 = \"Fruits\" #@param {type:\"string\"}\n",
        "Category2 = \"Angry Person\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown How many images of each category should we try to get?\n",
        "ImageCount = 200 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "yPKU5ggvsZl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hkO4YWa-UaX"
      },
      "outputs": [],
      "source": [
        "for category in [Category1, Category2]:\n",
        "    image_getter.download(category, limit=ImageCount,  output_dir=DataDirectory)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the data into python\n",
        "\n",
        "We now have our data downloaded, but we still need to load it into python so we can use it. \n",
        "Luckily, PyTorch gives us a helper for loading image data from folders: `torchvision.datasets.ImageFolder`"
      ],
      "metadata": {
        "id": "JRQYepj_4MNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torchvision.datasets.ImageFolder(DataDirectory)"
      ],
      "metadata": {
        "id": "bsX4DRMb8h7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This automatically loads our images and categories into python on demand. \n",
        "We can get information about our data as follows:"
      ],
      "metadata": {
        "id": "-y2daPIF8-cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'The categories are: {dataset.classes}')\n",
        "print(f'We got {len(dataset)} images!')"
      ],
      "metadata": {
        "id": "DC7NNDuV8h4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, our data still isn't loaded in the format we want. \n",
        "To get it in the correct format, we transform it to a PyTorch Tensor and resize and crop it to a consistent size. \n",
        "Luckily, `torchvision.transforms` gives us a convinient way of doing just this."
      ],
      "metadata": {
        "id": "SCGmPCBS-d8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = torchvision.transforms.Compose([\n",
        "        # Turn our image into a tensor\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        # Resize and crop our image to 224x224 randomly\n",
        "        torchvision.transforms.RandomResizedCrop(224),\n",
        "        # Randomly flip some of our images\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "dataset.transform = data_transforms"
      ],
      "metadata": {
        "id": "X6fKMvsr8h0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good practice to hold back some of our data to test the model with."
      ],
      "metadata": {
        "id": "Z5pVzBq4CCqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Split { display-mode: \"both\" }\n",
        "\n",
        "#@markdown How much of our data should be withheld for testing?\n",
        "Percent_Withheld = 15 #@param {type:\"number\"}"
      ],
      "metadata": {
        "id": "LyZD3PlAEezC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch provides a function to automatically split our dataset, but it needs to have the exact lengths of each split. \n",
        "We have to calculate these from our percent withheld as follows:"
      ],
      "metadata": {
        "id": "E1AW_IQdH1JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_size = len(dataset)\n",
        "test_size = int( (Percent_Withheld/100) * total_size )\n",
        "train_size = total_size - test_size"
      ],
      "metadata": {
        "id": "vurDnUFJHwzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we finally create our data splits"
      ],
      "metadata": {
        "id": "uQibVIWZISMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "                                    dataset, \n",
        "                                    lengths = [train_size, test_size]\n",
        "                                )"
      ],
      "metadata": {
        "id": "v3lVaF46IPEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to create DataLoaders to get our data in random batches."
      ],
      "metadata": {
        "id": "W6Eck5y8HSGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Batches { display-mode: \"both\" }\n",
        "\n",
        "#@markdown How many images do we want in each batch?\n",
        "batch_size = 16 #@param {type:\"number\"}"
      ],
      "metadata": {
        "id": "t74vZRW9FS3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    # Give us multiple images every time\n",
        "    batch_size = batch_size,\n",
        "    # Randomly shuffle the data so it's not in order\n",
        "    shuffle = True,\n",
        "    # Use background workers to load our data\n",
        "    num_workers = 2,\n",
        "    # Each worker should pre-fetch 2 images\n",
        "    prefetch_factor = 2,\n",
        "    # Use pinned memory for faster GPU loading\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset = test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = 2,\n",
        "    prefetch_factor = 2,\n",
        "    pin_memory = True,\n",
        ")"
      ],
      "metadata": {
        "id": "6Krpfv4zI2JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying our dataset\n",
        "\n",
        "It's nice to take a look at our data to make sure it loaded correctly. We can get a batch from our dataloader and display the images and categories."
      ],
      "metadata": {
        "id": "SlaE4JV3EXsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",\"Palette images with .*\")"
      ],
      "metadata": {
        "id": "NAQMDa13q-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_loader))"
      ],
      "metadata": {
        "id": "4G2mQZ9m8huu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time we get the next batch from our loader, it gives us two tensors: one with our images, and one containing their class labels."
      ],
      "metadata": {
        "id": "0pTwQ845LzzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Image tensor has shape {images.shape}')\n",
        "print(f'Label tensor has shape {labels.shape}')"
      ],
      "metadata": {
        "id": "3miTi_eeLnzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the images come in a 4D tensor: \\[batch, color, width, height\\], and our labels are a 1D tensor: \\[batch\\] \n",
        "\n",
        "Our label tensor is a list of integers, where the number is the index of the category. To get the category names, we can do the following:"
      ],
      "metadata": {
        "id": "Jk9JSvjDMdpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_labels = [ dataset.classes[index] for index in labels ]"
      ],
      "metadata": {
        "id": "qRfsTDvGLna_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to plot an image, we need to convert it to a numpy array and rearrange the dimensions so color is last. Since these are common tasks, we will create functions for them."
      ],
      "metadata": {
        "id": "7Gzpo8uqOXce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_np(tensor):\n",
        "    '''Convert a PyTorch Tensor to a NumPy array'''\n",
        "    # We need to let pytorch know we are taking the tensor\n",
        "    tensor = tensor.detach()\n",
        "    # We must put our tensor on the CPU if it is on the GPU\n",
        "    tensor = tensor.cpu()\n",
        "    # Finally, we can convert our tensor to a NumPy array\n",
        "    array = tensor.numpy()\n",
        "    return array\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    # We must convert our tensor to a NumPy array\n",
        "    array = to_np(tensor)\n",
        "    # We need to rearrange our array from [c,w,h] to [w,h,c]\n",
        "    array = array.transpose((1,2,0))\n",
        "    return array"
      ],
      "metadata": {
        "id": "xDh8HHdLPM-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can plot one of our images using matplotlib:"
      ],
      "metadata": {
        "id": "kgilIWkJSGfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure\n",
        "fig, ax = plt.subplots()\n",
        "# Show our first image\n",
        "ax.imshow( tensor_to_image(images[0]) )\n",
        "# Set the plot title to the image category\n",
        "ax.set_title( human_labels[0] )\n",
        "# Turn off the axis ticks\n",
        "ax.set_axis_off()\n",
        "# Show our image\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "gmmhtB5k8hrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the whole batch at once by creating multiple subplots"
      ],
      "metadata": {
        "id": "BrzxqrRWTk5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a grid of plots\n",
        "next_square = int(np.ceil(np.sqrt(batch_size)))\n",
        "fig, axes = plt.subplots(next_square, next_square, figsize=(10,10))\n",
        "\n",
        "# loop over our subplots with each image and label\n",
        "for ax, image, label in zip(axes.flat, images, human_labels):\n",
        "    # Show the image\n",
        "    ax.imshow( tensor_to_image(image) )\n",
        "    # Set the plot title to the image category\n",
        "    ax.set_title( label )\n",
        "    # Turn off the axis ticks\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# Show our image\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XZqHkgJyTb8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, our data isn't perfect, but it is often good enough."
      ],
      "metadata": {
        "id": "UsgDdqU2Vhoo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network"
      ],
      "metadata": {
        "id": "7yf2NR-6VpiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our dataset loaded into python, we can start training a model to classify our data."
      ],
      "metadata": {
        "id": "nJsqPp_PVybc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading a model\n",
        "\n",
        "Modern neural network models need a lot of data and a long time to train, but there is a way to cheat. In stead of training a model from scratch, we can start with a \"pre-trained\" model that has already been trained on a large dataset and fine-tune it to classify our own data."
      ],
      "metadata": {
        "id": "MR66m5YmW7oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch already has a [large library of pretrained classifier models](https://pytorch.org/vision/0.11.0/models.html#classification) ready for us to use. When choosing one, there is allways a tradeoff between the size, performance, and accuracy. In this tutorial, we will use MobileNet V3 Small for its small size low CPU inference time.\n",
        "\n",
        "**WARNING: *The procedure for loading pretrained models is changing. This code works in versions of torchvision between 0.9.0 and 0.12.0***"
      ],
      "metadata": {
        "id": "swyYvkhOY-qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.mobilenet_v3_small(pretrained=True)"
      ],
      "metadata": {
        "id": "fkm9nIXYaXFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can print our model to see the layer architecture. It may seem large and complex seeming, but we really don't need to worry about that.\n",
        "\n",
        "If you are interested in the details, you can get information [here](https://paperswithcode.com/lib/torchvision/mobilenet-v3), or read the paper [here](https://arxiv.org/abs/1905.02244)"
      ],
      "metadata": {
        "id": "UaVzZDrLfFsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "print(f'Model has {sum([len(p.flatten()) for p in model.parameters()]):,d} parameters')"
      ],
      "metadata": {
        "id": "Wr17MMvpe8zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model was trained on ImageNet, so it has 1000 output classes. To classify our data, we change the number of classes the last layer outputs."
      ],
      "metadata": {
        "id": "ns5TzOnJjlKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier[-1] = torch.nn.Linear(in_features=1024, out_features=len(dataset.classes))"
      ],
      "metadata": {
        "id": "C0jthW97iTEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we should put our model on the GPU if we have one."
      ],
      "metadata": {
        "id": "9TFb4kE0ksWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'We are using {device}')"
      ],
      "metadata": {
        "id": "zzJzNpBjkopx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "gxKnfnD2k8ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization Aware Training\n",
        "\n",
        "In order to make model inference faster, we will use quantization. However, quantization can impact performance due to rounding. These impacts can be mitigated with \"Quantization Aware Training\" (QAT), in which we model the effects of quantization during the training process."
      ],
      "metadata": {
        "id": "XdSIFlCWf_6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To set up our model for QAT, we must first add layers to convert the inputs and outputs to/from quantized tensors. This is can be done automatically by wrapping the model in a `torch.quantization.QuantWrapper`"
      ],
      "metadata": {
        "id": "FfXn6YHIg8AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_fakequant = torch.quantization.QuantWrapper(model)"
      ],
      "metadata": {
        "id": "R_XjoEBbgEI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we optimize the model by fusing certain layers together. Only some types of layers can be fused, and this varies from model to model."
      ],
      "metadata": {
        "id": "dzVY1b6fkAu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for block in model_fakequant.modules():\n",
        "    # We only can fuse ConvNormActivation blocks in this model\n",
        "    if type(block) is torchvision.ops.misc.ConvNormActivation:\n",
        "        if len(block) == 3 and type(block[2]) is torch.nn.ReLU:\n",
        "            # Block contains Conv, BN, ReLU. Fuse them all.\n",
        "            fuse_layers = [\"0\", \"1\", \"2\"]\n",
        "        else:\n",
        "            # Fuse Conv and BN, there is no ReLU.\n",
        "            fuse_layers = [\"0\", \"1\"]\n",
        "        torch.quantization.fuse_modules(block, fuse_layers, inplace=True)"
      ],
      "metadata": {
        "id": "IFcYqeT0iJaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to set the quantization backend. Since we will be running inference on ARM, we should use the `qnnpack` backend. For x86, we would use `fbgemm` instead."
      ],
      "metadata": {
        "id": "VhefqLFRLk2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.quantized.engine = 'qnnpack'\n",
        "model_fakequant.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')"
      ],
      "metadata": {
        "id": "Nm9dd5RGNOCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can prepare our model for quantization-aware training"
      ],
      "metadata": {
        "id": "eWl1UK8FK6Vl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.quantization.prepare_qat(model_fakequant, inplace=True)"
      ],
      "metadata": {
        "id": "YD-TNFCoKwv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up logging\n",
        "\n",
        "It is usefull to log your progress during training and interactively visualize it. To do this, we will use a platform called [Weights & Biases](https://wandb.ai/)."
      ],
      "metadata": {
        "id": "JHrqDurYmqvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you will need an account to use Weights & Biases. You can create a free account [here](https://app.wandb.ai/login?signup=true).\n",
        "\n",
        " If you do not wish to create an account, simply remove any lines using `wandb` and create your own config object by running the following:\n",
        "`config = type('config', (), {})()`."
      ],
      "metadata": {
        "id": "6hn5yONXqb6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We install the Weights & Biases client here\n",
        "# Do not run this if you already have it installed\n",
        "%pip install wandb"
      ],
      "metadata": {
        "id": "jx7zictKk-JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "jFJ0R5uxn5rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to log our training with Weights & Biases, we must initialize a new run. "
      ],
      "metadata": {
        "id": "Uf6K-8mioSWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"SoFlo DevCon Tutorial\")\n",
        "config = wandb.config\n",
        "# or: config = type('config', (), {})()"
      ],
      "metadata": {
        "id": "a5EkNtnvoQjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing to train\n",
        "\n",
        "There are a few more things we must set up before we can train our model."
      ],
      "metadata": {
        "id": "7Xb3crVolAGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have logging configured, we can set some hyperparameters and log them to W&B"
      ],
      "metadata": {
        "id": "dp-dCV_MsuG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Hyperparameters\n",
        "\n",
        "# Let's add the parameters we already chose for reference\n",
        "\n",
        "config.data_classes = dataset.classes\n",
        "\n",
        "config.num_train_images = len(train_dataset)\n",
        "config.num_test_images = len(test_dataset)\n",
        "\n",
        "config.batch_size = batch_size\n",
        "\n",
        "config.model_type = \"MobileNetV3 Small\"\n",
        "\n",
        "#@markdown What should our Learning Rate be?\n",
        "config.lr = 1e-3 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown How many times should we train over our data?\n",
        "config.epochs = 20 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "_0ieDBjVoQgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to configure an optimizer to train our model weights. There are many choices, but we wil use Adam."
      ],
      "metadata": {
        "id": "dOVbawq3JWof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(\n",
        "    params = model.parameters(),\n",
        "    lr = config.lr,\n",
        ")"
      ],
      "metadata": {
        "id": "2Ou9LfoaoQbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to choose a loss function. We will use Categorical Cross-Entropy, which is pretty standard for classification tasks."
      ],
      "metadata": {
        "id": "YN9FplWTKXqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "0PKt5RZkoQX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The training loop\n",
        "\n",
        "It's finally time to get down to business. The training loop will go through the training data and update our model weights, then evaluate our model's performance on the test data, log the results, and repeat for every epoch."
      ],
      "metadata": {
        "id": "NcaOq6KpLnPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training takes a while and only outputs at this url\n",
        "print(f\"View progress at {wandb.run.url}\")\n",
        "\n",
        "# Store the latest accuracy\n",
        "accuracy = 0\n",
        "\n",
        "# Make sure we can stop the training with Ctrl-C or the button\n",
        "try:\n",
        "\n",
        "    # Repeat the train loop for every epoch.\n",
        "    # Use trange from tqdm to show a progress bar.\n",
        "    # We want to start at 1 and go to (excluding) epochs+1.\n",
        "    for epoch in trange(1, config.epochs+1, unit=\"epochs\"):\n",
        "\n",
        "        # The training phase\n",
        "        model.train()\n",
        "\n",
        "        # Loop through the training data\n",
        "        for data, labels in train_loader:\n",
        "\n",
        "            # Move the data and labels to GPU if available\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            # Clear the model gradient computations\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Generate our predictions\n",
        "            output = model(data)\n",
        "            # The prediction is the position of the highest value\n",
        "            predicted_classes = output.max(1)[1]\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_func(output, labels)\n",
        "\n",
        "            # Backpropagate: compute the gradients\n",
        "            loss.backward()\n",
        "            # Update the model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute the average accuracy\n",
        "            train_accuracy = torch.sum(predicted_classes == labels) / len(data)\n",
        "\n",
        "            # Log our statistics to W&B\n",
        "            wandb.log({\n",
        "                'train': {\n",
        "                    'loss': loss.item(),\n",
        "                    'accuracy': train_accuracy.item(),\n",
        "                },\n",
        "            })\n",
        "        \n",
        "        # The testing phase\n",
        "        model.eval()\n",
        "\n",
        "        # Keep a running total of loss and correct answers\n",
        "        num_correct = 0\n",
        "        running_loss = 0\n",
        "\n",
        "        # Store some examples\n",
        "        examples = []\n",
        "\n",
        "        # Loop through the testing data\n",
        "        for data, labels in test_loader:\n",
        "\n",
        "            # Move the data and labels to GPU if available\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            # Do not compute gradients\n",
        "            with torch.no_grad():\n",
        "                # Generate our predictions\n",
        "                output = model(data)\n",
        "                # The prediction is the position of the highest value\n",
        "                predicted_classes = output.max(1)[1]\n",
        "\n",
        "                # Compute the loss\n",
        "                running_loss += loss_func(output, labels).item()\n",
        "\n",
        "            # Find how many predictions were correct\n",
        "            num_correct += torch.sum(predicted_classes == labels).item()\n",
        "\n",
        "            # Save an example image\n",
        "            examples.append(\n",
        "                wandb.Image( \n",
        "                    tensor_to_image(data[0]), \n",
        "                    caption = f\"Predicted: {dataset.classes[predicted_classes[0].item()]}\" \\\n",
        "                            + f\"; Actual: {dataset.classes[labels[0].item()]}\",\n",
        "                )\n",
        "            )\n",
        "        \n",
        "        # Compute the model accuracy\n",
        "        accuracy = num_correct / (len(test_loader) * config.batch_size)\n",
        "\n",
        "        # Log our statistics to W&B\n",
        "        wandb.log({\n",
        "            'epoch': epoch,\n",
        "            'test': {\n",
        "                'loss': running_loss / len(test_loader),\n",
        "                'accuracy': accuracy,\n",
        "                'examples': examples,\n",
        "            },\n",
        "        })\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    # If we interrupt the training, print stopped\n",
        "    print(\"Stopped\")\n",
        "\n",
        "# Print our final accuracy\n",
        "print(f\"Achieved test accuracy of {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "K7RwvrReoQU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "4ZC2VxNfrhVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the Model"
      ],
      "metadata": {
        "id": "hPo08Nzq8kk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our model is trained, we can compile it and export it to load onto our Raspberry Pi."
      ],
      "metadata": {
        "id": "epfPYFu3QRdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will convert our fake quantized model into a real quantized one. In order to do this, we must make sure it is on the CPU and set in inference mode."
      ],
      "metadata": {
        "id": "63uZkdrCQcUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.cpu()\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "ElDWsRDCSYYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = torch.quantization.convert(model, inplace=False)"
      ],
      "metadata": {
        "id": "o0wmj9S28ouL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can \"compile\" it to TorchScript using `torch.jit`"
      ],
      "metadata": {
        "id": "AzRvAtAeQxpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model = torch.jit.script(quantized_model)"
      ],
      "metadata": {
        "id": "wNqECK9oQ7QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can further optimize by freezing the model, which speeds up the model by inlining all of the submodule code and parameters into a single function, thereby removing the overhead of extra function calls and objects."
      ],
      "metadata": {
        "id": "7r2K9gFMS0WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model = torch.jit.freeze(compiled_model)"
      ],
      "metadata": {
        "id": "FGamkfbHQ7Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can save our compiled model to a file."
      ],
      "metadata": {
        "id": "1k9JVmMfT9fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.jit.save(compiled_model,'sfdc_tutorial_classifier.pth')"
      ],
      "metadata": {
        "id": "PaxuTPoUUDz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If on Google Colab, we can download the file with the below cell. Otherwise, do not run the cell and get the file manually."
      ],
      "metadata": {
        "id": "X2_gvMcFUPNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.files import download\n",
        "download('sfdc_tutorial_classifier.pth')"
      ],
      "metadata": {
        "id": "JMmCmnb-UN4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congradulations!! You have now trained and exported a model. It's time to get it on to your Raspberry Pi. Continue following the tutorial at <https://github.com/mpcrlab/sfdc2022_tutorial/blob/main/RPi_Setup.md> to set up your Pi."
      ],
      "metadata": {
        "id": "xyQHv4APUqn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References and Acknowledgements"
      ],
      "metadata": {
        "id": "sGGhhErq9LmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentation for the libraries used:\n",
        "\n",
        "*  PyTorch: <https://pytorch.org/docs/stable/index.html>\n",
        "*  Torchvision: <https://pytorch.org/vision/stable/>\n",
        "*  Matplotlib: <https://matplotlib.org/stable/api/index>\n",
        "*  Weights & Biases: <https://docs.wandb.ai/>\n",
        "*  Bing Image Downloader: <https://pypi.org/project/bing-image-downloader/>"
      ],
      "metadata": {
        "id": "RrWYTOPA9OLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relevant papers:\n",
        "\n",
        "*  Searching for MobileNetV3: [arXiv:1905.02244](https://arxiv.org/abs/1905.02244)\n",
        "*  Adam: A Method for Stochastic Optimization: [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)"
      ],
      "metadata": {
        "id": "NkT5750J-6rb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'd like to thank [South Florida Tech Hub](https://techhubsouthflorida.org/) for organizing [SoFlo Dev Con 2022](https://techhubsouthflorida.org/meetups/soflodevcon/) and giving me an opportunity to present this workshop.\n",
        "\n",
        "I'd also like to acknowledge my collegues at the [MPCR Lab](https://mpcrlab.com/) and the [Rubin and Cindy Gruber Sandbox](https://www.fau.edu/sandbox/) at Florida Atlantic University."
      ],
      "metadata": {
        "id": "kA6asS47_ufe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Legal Nonsense"
      ],
      "metadata": {
        "id": "E5Gq0-AQBXUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Permission to use, copy, modify, and/or distribute these materials for any purpose with or without fee and without restriction is hereby granted. Attribution is appreciated, but is not necessary."
      ],
      "metadata": {
        "id": "9eJnarEKEEYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To the best of the authors' knowledge, the materials provided here are functional, free of malware, and do not infringe on any rights of any persons, legal or otherwise. However, the authors do not warrant or guarantee against any damages to any persons or property caused by or in relation to these materials."
      ],
      "metadata": {
        "id": "QhL0JGIHFKma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer: \n",
        "\n",
        "All materials, including but not limited to any software, source code, and instructions, in this tutorial are provided \"AS IS\", without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and noninfringement. In no event shall the authors or contributors be held liable for any claim, damages, or other liability, whether in action of contract, tort, or otherwise, arising from, out of, or in connection with these materials or the use of or other dealings in these materials."
      ],
      "metadata": {
        "id": "s_xc26gTBNt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For those of you who do not speak legalese:\n",
        "\n",
        "1.  Do whatever you want with this.\n",
        "3.  We don't think you'll have any problems.\n",
        "2.  If you do, though, it's not our fault."
      ],
      "metadata": {
        "id": "ph1moXOZGEm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authors and Contributors"
      ],
      "metadata": {
        "id": "5aDzy7BeHNGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Misha Klopukh"
      ],
      "metadata": {
        "id": "20fUVg6nHQNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* William Edward Hahn"
      ],
      "metadata": {
        "id": "J8MM5AYTHUI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You? Contribute by opening a pull request at <https://github.com/mpcrlab/sfdc2022_tutorial/pulls>."
      ],
      "metadata": {
        "id": "iGlNVxYoHZn3"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "SoFloDevCon_Tutorial_2022.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
